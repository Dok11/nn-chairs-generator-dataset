import gc
import os

import h5py
import numpy as np
from keras import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D, Flatten
from keras.models import load_model
from keras.optimizers import SGD
from keras_preprocessing.image import ImageDataGenerator

BATCH_SIZE = 128
NUM_CLASSES = 6
EPOCHS = 10
SAVE_DIR = os.path.join(os.getcwd(), '..', 'models', 'main.h5')
USE_SAVED_MODEL: bool = False
SLICE_COUNT: int = 2
SLICE_PART: str = '000'


def get_dataset(slice_part: str):
    test_file_path = os.path.join(os.getcwd(), '..', 'data', 'validation.h5')
    #test_file_path = os.path.join(os.getcwd(), '..', 'data', 'test' + slice_part + '.h5')
    train_file_path = os.path.join(os.getcwd(), '..', 'data', 'train' + slice_part + '.h5')

    with h5py.File(test_file_path, 'r') as hf:
        test_data = hf['data'][:]
        test_label = hf['label'][:]

    with h5py.File(train_file_path, 'r') as hf:
        train_data = hf['train_data'][:]
        train_label = hf['train_label'][:]

    return (train_data, train_label), (test_data, test_label)


def get_datagen():
    return ImageDataGenerator(
                rotation_range=40,
                vertical_flip=True,
                width_shift_range=.3,
                height_shift_range=.3,
                shear_range=.2,
                zoom_range=.2,
                fill_mode='nearest',
                rescale=1. / 255,
            )


def get_validation_datagen():
    return ImageDataGenerator(
                rescale=1./255,
            )


if USE_SAVED_MODEL:
    model = load_model(SAVE_DIR)

    for x in range(SLICE_COUNT):
        print('Start train', x + 1, 'from', SLICE_COUNT)
        (x_train, y_train), (x_test, y_test) = get_dataset(str(x).zfill(3))

        datagen = get_datagen()
        datagen.fit(x_train)
        generator = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)

        # Fit the model on the batches generated by datagen.flow().
        model.fit_generator(generator,
                            steps_per_epoch=100,
                            workers=2,
                            epochs=EPOCHS,
                            validation_data=(x_test, y_test),
                            shuffle=True)

        # Score trained model.
        scores = model.evaluate(x_test, y_test, verbose=2)
        print('Test loss:', scores[0])
        print('Test accuracy:', scores[1])

        del datagen
        del generator
        del x_train
        del y_train
        del x_test
        del y_test
        gc.collect()

else:
    (x_train, y_train), (x_test, y_test) = get_dataset(SLICE_PART)

    print('x_train shape:', x_train.shape)
    print(x_train.shape[0], 'train samples')
    print(x_test.shape[0], 'test samples')

    model = Sequential()

    model.add(Conv2D(12, (3, 3), input_shape=x_train.shape[1:], activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))  # 255 -> 128

    model.add(Conv2D(24, (3, 3), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))  # 127 -> 64
    model.add(Dropout(0.1))

    model.add(Conv2D(48, (3, 3), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))  # 63 -> 32
    model.add(Dropout(0.1))

    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))  # 31 -> 16
    model.add(Dropout(0.1))

    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))  # 15 -> 8
    model.add(Dropout(0.1))

    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(NUM_CLASSES, activation='sigmoid'))

    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

    datagen = get_datagen()
    datagen.fit(x_train)
    generator = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)

    generator_datagen = get_validation_datagen()
    generator_datagen.fit(x_test)
    generator_validation = generator_datagen.flow(x_test, y_test)

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(generator,
                        steps_per_epoch=100,
                        workers=2,
                        epochs=EPOCHS,
                        validation_data=generator_validation,
                        shuffle=True)

    # Score trained model.
    scores = model.evaluate(x_test, y_test, verbose=2)
    print('Test loss:', scores[0])
    print('Test accuracy:', scores[1])


model.summary()

# Save model and weights
if scores[1] > 0.1:
    model.save(SAVE_DIR)
    print('Saved trained model at %s ' % SAVE_DIR)
