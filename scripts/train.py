import gc
import os

import h5py
from keras.applications.inception_v3 import InceptionV3
from keras.layers import Dense, GlobalAveragePooling2D
from keras.models import load_model, Model
from keras.optimizers import SGD
from keras_preprocessing.image import ImageDataGenerator

BATCH_SIZE = 48
NUM_CLASSES = 6
REMOVE_COLUMNS = []  # [1, 2, 3, 4, 5]
EPOCHS = 5
SAVE_DIR = os.path.join(os.getcwd(), '..', 'models', 'main.h5')
USE_SAVED_MODEL: bool = True
SLICE_COUNT: int = 3
SLICE_PART: str = '000'


def unselect_column(L, L1):
    new_dict = []
    for i in range(len(L)):
        new_dict.append([])  # create nested list
        for j in range(len(L[i])):
            if j not in L1:
                new_dict[i].append(L[i][j])

    return new_dict


def get_dataset(slice_part: str):
    test_file_path = os.path.join(os.getcwd(), '..', 'data', 'validation.h5')
    train_file_path = os.path.join(os.getcwd(), '..', 'data', 'train' + slice_part + '.h5')

    with h5py.File(test_file_path, 'r') as hf:
        test_data = hf['data'][:]
        test_label = hf['label'][:]
        test_label = unselect_column(test_label, REMOVE_COLUMNS)
        # test_label = keras.utils.to_categorical(test_label, NUM_CLASSES)

    with h5py.File(train_file_path, 'r') as hf:
        train_data = hf['train_data'][:]
        train_label = hf['train_label'][:]
        train_label = unselect_column(train_label, REMOVE_COLUMNS)
        # train_label = keras.utils.to_categorical(train_label, NUM_CLASSES)

    return (train_data, train_label), (test_data, test_label)


def get_datagen():
    return ImageDataGenerator(
                rotation_range=20,
                vertical_flip=True,
                width_shift_range=.25,
                height_shift_range=.25,
                shear_range=.2,
                zoom_range=.2,
                fill_mode='nearest',
                rescale=1. / 255,
            )


def get_validation_datagen():
    return ImageDataGenerator(rescale=1./255)


(x_train, y_train), (x_test, y_test) = get_dataset(SLICE_PART)


# print('x_test shape:', x_test.shape)
# print('y_test shape:', y_test.shape)
# print(x_train.shape[0], 'train samples')
# print(x_test.shape[0], 'test samples')
# exit()


if USE_SAVED_MODEL:
    model = load_model(SAVE_DIR)

    for x in range(SLICE_COUNT):
        print('Start train', x + 1, 'from', SLICE_COUNT)
        (x_train, y_train), (x_test, y_test) = get_dataset(str(x).zfill(3))

        datagen = get_datagen()
        datagen.fit(x_train)
        generator = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)

        generator_datagen = get_validation_datagen()
        generator_datagen.fit(x_test)
        generator_validation = generator_datagen.flow(x_test, y_test)

        # Fit the model on the batches generated by datagen.flow().
        model.fit_generator(generator,
                            epochs=EPOCHS,
                            validation_data=generator_validation,
                            shuffle=True)

        del datagen
        del generator
        del x_train
        del y_train
        del x_test
        del y_test
        gc.collect()

else:
    print('x_train shape:', x_train.shape)
    print(x_train.shape[0], 'train samples')
    print(x_test.shape[0], 'test samples')

    datagen = get_datagen()
    datagen.fit(x_train)
    generator = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)

    generator_datagen = get_validation_datagen()
    generator_datagen.fit(x_test)
    generator_validation = generator_datagen.flow(x_test, y_test)


    # inception
    base_model = InceptionV3(weights='imagenet',
                             include_top=False,
                             input_shape=(150, 150, 3))

    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(2048, activation='relu')(x)
    predictions = Dense(NUM_CLASSES, activation='sigmoid')(x)

    model = Model(inputs=base_model.input, outputs=predictions)

    # first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional InceptionV3 layers
    for layer in base_model.layers:
        layer.trainable = False

    # compile the model (should be done *after* setting layers to non-trainable)
    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])

    # train the model on the new data for a few epochs
    model.fit_generator(generator,
                        epochs=2,
                        validation_data=generator_validation,
                        shuffle=True)

    # at this point, the top layers are well trained and we can start fine-tuning
    # convolutional layers from inception V3. We will freeze the bottom N layers
    # and train the remaining top layers.

    # let's visualize layer names and layer indices to see how many layers
    # we should freeze:
    for i, layer in enumerate(base_model.layers):
        print(i, layer.name)

    # we chose to train the top 2 inception blocks, i.e. we will freeze
    # the first 249 layers and unfreeze the rest:
    for layer in model.layers[:249]:
        layer.trainable = False
    for layer in model.layers[249:]:
        layer.trainable = True

    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9),
                  loss='binary_crossentropy',
                  metrics=['binary_accuracy'])


    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(generator,
                        epochs=EPOCHS,
                        validation_data=generator_validation,
                        shuffle=True)

# model.summary()

# Save model and weights
# if scores[1] > 0.1:
model.save(SAVE_DIR)
print('Saved trained model at %s ' % SAVE_DIR)
